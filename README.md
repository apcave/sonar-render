# sonar-render
The software renders and visualizes acoustic waves traveling in 3D space, reflecting off target objects. Observations can be recorded numerically as complex pressure values using field points. Visual observations can be made using field surfaces, where the complex pressure at a point on the surface is rendered. Additionally, visual observations can be made of the pressure on the surface of a target object. The field objects measure the pressure and are acoustically transparent. Sound can be projected through monopole point sources or as coherent pressure across a source object. Source objects are designed to simulate piston-like piezo sources. Multiple reflections can be simulated as sound is projected from one surface to another, similar to what occurs inside a room. The model employs Kirchhoff wave equations with Green's functions to perform integrations. The medium containing the objects can be altered in terms of wave speed and attenuation, and the frequency of the source can also be adjusted. The model is effective across a wide range of frequencies, although care must be taken to ensure that the size of the discrete fragment area is appropriate for capturing phase-related information. Acoustic waves are visualized using a three-pole color wheel of red, blue, and green to represent the 360-degree value of the phase of the pressure wave at each point. The amplitude of the pressure wave at a point is visualized using brightness scaled to decibels, with the brightest values at -20 dB and the smallest visible amplitude at -90 dB, providing a dynamic range of 70 dB. The brightness is fixed in the code, allowing for visual comparison of the intensity of the sound waves in the renders. The scenes are rendered as PNG image files, with the camera location and 3D geometry set in Python code.
### Current Limitations
Boundary conditions are rigid or represent a vacuum, although there are plans to include composite material layers or coatings. Currently, all sound is reflected, and there is no transmission; therefore, acoustically translucent materials are not modeled.

# Target Platform
The rendering of acoustic waves is significantly more computationally intensive compared to 3D computer graphics. The main difference lies in the fact that sound waves, which are low-frequency electromagnetic waves, scatter spherically from points rather than reflecting as rays at angles of incidence. Therefore, if a surface consists of ( n ) points with one source, the time complexity of the algorithm is ( $O(n)$ ). For a single reflection, it becomes ( $O(n^2)$ ), and for two reflections, it is ( $O(n^3)$ ). As a result, calculations are performed on static scenes rather than in real-time with moving objects.

The software utilizes NVIDIA graphics cards along with associated OptiX and CUDA technology. Rays travel from point to point, and OptiX is employed to determine if there is an obstacle in the path that causes a shadow. CUDA is used to calculate the phase and intensity of the acoustic waves traveling from point to point, point to fragment, fragment to fragment, or fragment to point. The values at the fragments are used to form textures over facets, which are then rendered using OpenGL and EGA technology. OpenGL shaders are utilized with arrays calculated using CUDA, incorporating interpolation between points. The fragment size and area are adjustable and should be set to a minimum fragment length of wavelength/7.5. Geometry is rendered as triangular facets composed of small squares of fragments.

The software can be run on a computer equipped with an NVIDIA graphics card running Linux. An older PC with an NVIDIA 1080 TI graphics card was used for development, while renders are currently being performed on AWS EC2 instances with much more powerful and modern graphics cards. The intent is to conduct scene renders using Docker containers and AWS spot instances.